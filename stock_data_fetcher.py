#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é‡åŒ–äº¤æ˜“ç³»ç»Ÿ - å›æµ‹æ•°æ®å‡†å¤‡è„šæœ¬
æ–‡ä»¶å: stock_data_fetcher.py
è™šæ‹Ÿç¯å¢ƒ: quant_trading
Pythonç‰ˆæœ¬: 3.10.x

ä¸“æ³¨äºä¸ºå›æµ‹å‡†å¤‡å¿…è¦çš„å†å²è‚¡ç¥¨æ•°æ®ï¼Œæ”¯æŒåŸºæœ¬é¢åˆ†æå’Œç­–ç•¥å›æµ‹
"""

import sys
import tushare as ts
import pymysql
import pandas as pd
import json
import os
import time
from datetime import datetime, timedelta, date
import logging
import traceback
from typing import List, Dict, Any, Optional, Tuple
import argparse


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# æ•°æ®åº“é»˜è®¤è®¾ç½®
DB_DEFAULTS = {
    "host": "localhost",
    "port": 3306,
    "user": "root",
    "database": "quantitative_trading",
    "charset": "utf8mb4",
}


class StockDataManager:
    """è‚¡ç¥¨æ•°æ®ç®¡ç†ç±»"""

    def __init__(
        self,
        db_password: str,
        tushare_token: str,
        start_date: str,
        end_date: str,
    ):
        """
        åˆå§‹åŒ–è‚¡ç¥¨æ•°æ®ç®¡ç†å™¨

        Args:
            db_password: æ•°æ®åº“å¯†ç 
            tushare_token: Tushare APIä»¤ç‰Œ
            start_date: å›æµ‹å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: å›æµ‹ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        """
        self.db_password = db_password
        self.connection = None
        self.start_date = start_date
        self.end_date = end_date

        # è½¬æ¢ä¸ºtushareæ ¼å¼çš„æ—¥æœŸ (YYYYMMDD)
        self.start_date_ts = (
            self.start_date.replace("-", "") if self.start_date else None
        )
        self.end_date_ts = self.end_date.replace("-", "") if self.end_date else None

        # åˆå§‹åŒ–Tushare
        ts.set_token(tushare_token)
        self.pro = ts.pro_api()
        self.logger = logger

    def connect_database(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.logger.info("è¿æ¥æ•°æ®åº“...")

            # è·å–é»˜è®¤è½¬æ¢å™¨å¹¶è¿›è¡Œè‡ªå®šä¹‰
            conv = pymysql.converters.conversions.copy()
            conv[datetime.date] = pymysql.converters.escape_date
            conv[pymysql.FIELD_TYPE.DECIMAL] = float
            conv[pymysql.FIELD_TYPE.NEWDECIMAL] = float

            # ä½¿ç”¨æ ‡å‡†è¿æ¥æ–¹å¼
            self.connection = pymysql.connect(
                host=DB_DEFAULTS["host"],
                port=DB_DEFAULTS["port"],
                user=DB_DEFAULTS["user"],
                password=self.db_password,
                database=DB_DEFAULTS["database"],
                charset=DB_DEFAULTS["charset"],
                autocommit=False,
                conv=conv,
            )

            self.logger.info("æ•°æ®åº“è¿æ¥æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"è¿æ¥æ•°æ®åº“å¤±è´¥: {e}")
            raise

    def close_database(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.connection:
            self.connection.close()
            self.logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")

    def initialize_trade_dates(self):
        """åˆå§‹åŒ–äº¤æ˜“æ—¥å†"""
        try:
            self.logger.info(f"åˆå§‹åŒ–ä»{self.start_date}åˆ°{self.end_date}çš„äº¤æ˜“æ—¥å†...")
            df = self.pro.trade_cal(
                exchange="SSE",
                start_date=self.start_date_ts,
                end_date=self.end_date_ts,
                is_open=1,
            )
            if df.empty:
                self.logger.warning(f"æœªè·å–åˆ°äº¤æ˜“æ—¥ä¿¡æ¯")
                self.trade_dates = []
            else:
                self.trade_dates = df["cal_date"].tolist()
                self.logger.info(f"æˆåŠŸåˆå§‹åŒ–{len(self.trade_dates)}ä¸ªäº¤æ˜“æ—¥")
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–äº¤æ˜“æ—¥å†å¤±è´¥: {e}")
            self.trade_dates = []

    def get_stock_data(
        self,
        ts_codes: List[str],
    ) -> pd.DataFrame:
        """è·å–è‚¡ç¥¨è¡Œæƒ…æ•°æ®"""
        try:
            ts_code_str = ",".join(ts_codes)
            self.logger.info(
                f"è·å–è‚¡ç¥¨{ts_code_str}ä»{self.start_date}åˆ°{self.end_date}çš„æ•°æ®"
            )
            df = self.pro.daily(
                ts_code=ts_code_str,
                start_date=self.start_date_ts,
                end_date=self.end_date_ts,
            )

            if df.empty:
                self.logger.warning("æœªè·å–åˆ°è‚¡ç¥¨æ•°æ®")
                return pd.DataFrame()

            self.logger.info(f"æˆåŠŸè·å–{len(df)}æ¡è‚¡ç¥¨æ•°æ®")
            return df
        except Exception as e:
            self.logger.error(f"è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def process_stock_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†è‚¡ç¥¨æ•°æ®ï¼Œè½¬æ¢ä¸ºæ•°æ®åº“æ ¼å¼"""
        if df.empty:
            return df

        # é‡å‘½ååˆ—
        column_mapping = {
            "ts_code": "stock_code",
            "trade_date": "trade_date",
            "open": "open_price",
            "high": "high_price",
            "low": "low_price",
            "close": "close_price",
            "pre_close": "pre_close_price",
            "change": "change_amount",
            "pct_chg": "change_percent",
            "vol": "volume",
            "amount": "amount",
        }

        processed_df = df.rename(columns=column_mapping)

        # æ•°æ®ç±»å‹è½¬æ¢
        processed_df["trade_date"] = pd.to_datetime(processed_df["trade_date"]).dt.date
        processed_df["change_percent"] = processed_df["change_percent"] / 100
        processed_df["volume"] = processed_df["volume"] * 100
        processed_df["amount"] = processed_df["amount"] * 1000

        # æ·»åŠ å…ƒæ•°æ®
        processed_df["data_source"] = "tushare"
        processed_df["collect_time"] = datetime.now()

        # å¤„ç†ç¼ºå¤±å€¼
        processed_df = processed_df.fillna(0)

        # é€‰æ‹©éœ€è¦çš„åˆ—
        required_columns = [
            "stock_code",
            "trade_date",
            "open_price",
            "high_price",
            "low_price",
            "close_price",
            "pre_close_price",
            "change_amount",
            "change_percent",
            "volume",
            "amount",
            "data_source",
            "collect_time",
        ]

        processed_df = processed_df[required_columns]
        self.logger.info(f"æ•°æ®å¤„ç†å®Œæˆï¼Œå¤„ç†äº†{len(processed_df)}æ¡è®°å½•")
        return processed_df

    def insert_stock_data(self, df: pd.DataFrame) -> int:
        """æ‰¹é‡æ’å…¥è‚¡ç¥¨æ•°æ®"""
        if df.empty:
            self.logger.warning("æ²¡æœ‰æ•°æ®éœ€è¦æ’å…¥")
            return 0

        try:
            cursor = self.connection.cursor()

            insert_sql = """
            INSERT INTO StockMarketData (
                stock_code, trade_date, open_price, high_price, low_price,
                close_price, pre_close_price, change_amount, change_percent,
                volume, amount, data_source, collect_time
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
            ) ON DUPLICATE KEY UPDATE
                open_price = VALUES(open_price),
                high_price = VALUES(high_price),
                low_price = VALUES(low_price),
                close_price = VALUES(close_price),
                pre_close_price = VALUES(pre_close_price),
                change_amount = VALUES(change_amount),
                change_percent = VALUES(change_percent),
                volume = VALUES(volume),
                amount = VALUES(amount),
                collect_time = VALUES(collect_time)
            """

            # æ‰¹é‡å¤„ç†æ•°æ®
            batch_size = 1000
            total_inserted = 0

            for i in range(0, len(df), batch_size):
                batch_df = df.iloc[i : i + batch_size]
                data_list = [tuple(row) for _, row in batch_df.iterrows()]

                cursor.executemany(insert_sql, data_list)
                batch_inserted = cursor.rowcount
                total_inserted += batch_inserted

            self.connection.commit()
            self.logger.info(f"æˆåŠŸæ’å…¥/æ›´æ–°{total_inserted}æ¡è‚¡ç¥¨æ•°æ®")
            return total_inserted

        except Exception as e:
            if self.connection:
                self.connection.rollback()
            self.logger.error(f"æ’å…¥è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")
            return 0
        finally:
            if cursor:
                cursor.close()

    def check_stock_data_completeness(self, stock_code: str) -> Tuple[float, int, int]:
        """æ£€æŸ¥æŒ‡å®šè‚¡ç¥¨åœ¨å›æµ‹æœŸé—´çš„æ•°æ®å®Œæ•´æ€§"""
        try:
            cursor = self.connection.cursor()

            # è·å–è¯¥æ—¶é—´æ®µå†…åº”æœ‰çš„äº¤æ˜“æ—¥
            trade_dates = self.get_available_trade_dates()
            expected_days = len(trade_dates)

            if expected_days == 0:
                return 0.0, 0, 0

            # æŸ¥è¯¢å®é™…æœ‰æ•°æ®çš„äº¤æ˜“æ—¥
            query = """
            SELECT COUNT(DISTINCT trade_date) 
            FROM StockMarketData 
            WHERE stock_code = %s 
            AND trade_date BETWEEN %s AND %s
            """
            cursor.execute(query, (stock_code, self.start_date, self.end_date))
            actual_days = cursor.fetchone()[0]

            # è®¡ç®—å®Œæ•´æ€§æ¯”ç‡
            completeness_ratio = (
                actual_days / expected_days if expected_days > 0 else 0.0
            )

            return completeness_ratio, actual_days, expected_days

        except Exception as e:
            self.logger.error(f"æ£€æŸ¥æ•°æ®å®Œæ•´æ€§å¤±è´¥: {e}")
            return 0.0, 0, 0
        finally:
            if cursor:
                cursor.close()

    def prepare_backtest_data(
        self,
        stock_codes: List[str],
        data_check_threshold: float = 0.9,  # æ•°æ®å®Œæ•´æ€§é˜ˆå€¼
        batch_query_interval: int = 1,  # APIè°ƒç”¨é—´éš”
    ) -> Dict[str, Any]:
        """å‡†å¤‡å›æµ‹æ‰€éœ€çš„å†å²æ•°æ®"""

        self.logger.info(f"å‡†å¤‡ä»{self.start_date}åˆ°{self.end_date}çš„å›æµ‹æ•°æ®")

        # æ‰¹å¤„ç†è‚¡ç¥¨æ•°æ®è·å–
        batch_size = 5  # Tushare APIæœ‰é¢‘ç‡é™åˆ¶ï¼Œæ¯æ¬¡è·å–å°‘é‡è‚¡ç¥¨

        result_stats = {
            "total_stocks": len(stock_codes),
            "processed_stocks": 0,
            "added_data_points": 0,
            "failed_stocks": [],
            "success_stocks": [],
            "start_date": self.start_date,
            "end_date": self.end_date,
        }

        # è·å–äº¤æ˜“æ—¥å†
        trade_dates = self.get_available_trade_dates()
        expected_days = len(trade_dates)
        result_stats["expected_trading_days"] = expected_days

        self.logger.info(f"è¯¥æ—¶é—´æ®µå†…åº”æœ‰{expected_days}ä¸ªäº¤æ˜“æ—¥")

        for i in range(0, len(stock_codes), batch_size):
            batch_stocks = stock_codes[i : i + batch_size]
            self.logger.info(
                f"å¤„ç†ç¬¬{i//batch_size + 1}æ‰¹ï¼Œè‚¡ç¥¨: {', '.join(batch_stocks)}"
            )

            for stock in batch_stocks:
                try:
                    # æ£€æŸ¥ç°æœ‰æ•°æ®å®Œæ•´æ€§
                    completeness, actual_days, _ = self.check_stock_data_completeness(
                        stock
                    )

                    if completeness >= data_check_threshold:
                        self.logger.info(
                            f"è‚¡ç¥¨{stock}æ•°æ®å®Œæ•´æ€§è‰¯å¥½({completeness:.2%})ï¼Œæ— éœ€æ›´æ–°"
                        )
                        result_stats["success_stocks"].append(stock)
                        result_stats["processed_stocks"] += 1
                        continue

                    # è·å–æ•°æ®å¹¶å¤„ç†
                    self.logger.info(
                        f"è·å–è‚¡ç¥¨{stock}çš„å†å²æ•°æ®ï¼Œå®Œæ•´æ€§: {completeness:.2%}"
                    )
                    raw_data = self.get_stock_data(ts_codes=[stock])

                    if raw_data.empty:
                        self.logger.warning(
                            f"æœªè·å–åˆ°è‚¡ç¥¨{stock}çš„æ•°æ®ï¼Œå¯èƒ½æ˜¯æ–°ä¸Šå¸‚æˆ–å·²é€€å¸‚"
                        )
                        result_stats["failed_stocks"].append(stock)
                        continue

                    processed_data = self.process_stock_data(raw_data)
                    inserted_count = self.insert_stock_data(processed_data)

                    # æ›´æ–°ç»Ÿè®¡
                    result_stats["added_data_points"] += inserted_count
                    result_stats["processed_stocks"] += 1
                    result_stats["success_stocks"].append(stock)

                except Exception as e:
                    self.logger.error(f"å¤„ç†è‚¡ç¥¨{stock}å¤±è´¥: {e}")
                    result_stats["failed_stocks"].append(stock)

                # æ·»åŠ APIè°ƒç”¨é—´éš”ï¼Œé¿å…è¶…è¿‡é¢‘ç‡é™åˆ¶
                time.sleep(batch_query_interval)

        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡æ•°æ®
        result_stats["success_rate"] = (
            len(result_stats["success_stocks"]) / result_stats["total_stocks"]
            if result_stats["total_stocks"] > 0
            else 0
        )

        return result_stats

    def get_stock_basic(self) -> int:
        """è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯"""
        try:
            self.logger.info("è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯...")
            df = self.pro.stock_basic(
                exchange="",
                list_status="L",
                fields="ts_code,name,area,industry,market,list_status,list_date,is_hs",
            )

            if df.empty:
                self.logger.warning("æœªè·å–åˆ°è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯")
                return 0

            # é‡å‘½ååˆ—
            df = df.rename(columns={"ts_code": "stock_code", "name": "stock_name"})

            # å¤„ç†æ—¥æœŸæ ¼å¼
            if "list_date" in df.columns:
                df["list_date"] = pd.to_datetime(df["list_date"]).dt.date

            # æ·»åŠ æ•°æ®æºå’Œé‡‡é›†æ—¶é—´
            df["data_source"] = "tushare"
            df["collect_time"] = datetime.now()

            # æ’å…¥æ•°æ®åº“
            return self.insert_data(df, "StockBasic")
        except Exception as e:
            self.logger.error(f"è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
            return 0

    def get_historical_stock_valuation(self) -> int:
        """
        è·å–å›æµ‹æœŸé—´æ¯ä¸ªäº¤æ˜“æ—¥çš„è‚¡ç¥¨ä¼°å€¼æ•°æ®

        Returns:
            æ’å…¥/æ›´æ–°çš„è®°å½•æ•°
        """

        # è·å–å›æµ‹æœŸé—´çš„æ‰€æœ‰äº¤æ˜“æ—¥
        self.logger.info(
            f"è·å–ä» {self.start_date} åˆ° {self.end_date} çš„å†å²è‚¡ç¥¨ä¼°å€¼æ•°æ®..."
        )
        trade_dates = self.get_available_trade_dates()

        if not trade_dates:
            self.logger.warning(
                f"åœ¨ {self.start_date} è‡³ {self.end_date} æœŸé—´æœªæ‰¾åˆ°äº¤æ˜“æ—¥"
            )
            return 0

        total_records = 0
        # ä¸ºæ¯ä¸ªäº¤æ˜“æ—¥è·å–ä¼°å€¼æ•°æ®
        for trade_date in trade_dates:
            self.logger.info(f"è·å– {trade_date} çš„ä¼°å€¼æ•°æ®...")
            records = self.get_stock_valuation(trade_date)
            total_records += records

            # æ·»åŠ APIè°ƒç”¨å»¶è¿Ÿï¼Œé¿å…è¶…è¿‡é¢‘ç‡é™åˆ¶
            time.sleep(1)

        self.logger.info(f"æˆåŠŸè·å–å¹¶å­˜å‚¨äº† {total_records} æ¡å†å²ä¼°å€¼æ•°æ®")
        return total_records

    def get_stock_valuation(self, trade_date) -> int:
        """
        è·å–ç‰¹å®šæ—¥æœŸçš„è‚¡ç¥¨ä¼°å€¼æ•°æ®

        Args:
            trade_date: äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼YYYYMMDD

        Returns:
            æ’å…¥/æ›´æ–°çš„è®°å½•æ•°
        """
        try:
            self.logger.info(f"è·å–{trade_date}çš„è‚¡ç¥¨ä¼°å€¼æ•°æ®...")
            df = self.pro.daily_basic(
                trade_date=trade_date,
                fields="ts_code,trade_date,pe,pb,ps,total_mv,circ_mv,turnover_rate",
            )

            if df.empty:
                self.logger.warning(f"æœªè·å–åˆ°{trade_date}çš„è‚¡ç¥¨ä¼°å€¼æ•°æ®")
                return 0

            # é‡å‘½ååˆ—
            df = df.rename(
                columns={
                    "ts_code": "stock_code",
                    "pe": "pe_ratio",
                    "pb": "pb_ratio",
                    "ps": "ps_ratio",
                    "total_mv": "market_cap",
                    "circ_mv": "circulating_market_cap",
                    "turnover_rate": "turnover_ratio",
                }
            )

            # è½¬æ¢æ—¥æœŸæ ¼å¼
            df["trade_date"] = pd.to_datetime(df["trade_date"]).dt.date

            # æ·»åŠ æ•°æ®æºå’Œé‡‡é›†æ—¶é—´
            df["data_source"] = "tushare"
            df["collect_time"] = datetime.now()

            # æ’å…¥æ•°æ®åº“
            return self.insert_data(df, "StockValuation")
        except Exception as e:
            self.logger.error(f"è·å–è‚¡ç¥¨ä¼°å€¼æ•°æ®å¤±è´¥: {e}")
            return 0

    def get_index_component(self, index_code="000300.SH") -> int:
        """è·å–æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®"""
        try:
            self.logger.info(f"è·å–{index_code}çš„æˆåˆ†è‚¡æ•°æ®...")

            # ä½¿ç”¨ä¸€ä¸ªå·²çŸ¥å­˜åœ¨æ•°æ®çš„å†å²æœˆä»½ï¼ˆå›ºå®šä¸º2023å¹´æœ€åä¸€ä¸ªå®Œæ•´æœˆä»½ï¼‰
            # è¿™æ ·ä¿è¯æ•°æ®ç¨³å®šå¯ç”¨
            start_date = "20231201"
            end_date = "20231231"

            self.logger.info(f"ä½¿ç”¨æ—¥æœŸèŒƒå›´ï¼š{start_date}è‡³{end_date}")

            # æŒ‰æ–‡æ¡£å»ºè®®ä¼ å…¥æœˆåº¦å¼€å§‹å’Œç»“æŸæ—¥æœŸ
            df = self.pro.index_weight(
                index_code=index_code, start_date=start_date, end_date=end_date
            )

            if df.empty:
                self.logger.warning(f"æœªè·å–åˆ°{index_code}çš„æˆåˆ†è‚¡æ•°æ®")

                # å°è¯•å¦ä¸€ç§æŒ‡æ•°ä»£ç æ ¼å¼ï¼ˆæ²ªæ·±300å¯èƒ½æœ‰ä¸¤ç§ä»£ç è¡¨ç¤ºï¼‰
                alt_code = "399300.SZ" if index_code == "000300.SH" else index_code
                if alt_code != index_code:
                    self.logger.info(f"å°è¯•ä½¿ç”¨æ›¿ä»£æŒ‡æ•°ä»£ç : {alt_code}")
                    df = self.pro.index_weight(
                        index_code=alt_code, start_date=start_date, end_date=end_date
                    )

                if df.empty:
                    self.logger.warning(
                        f"ä»æœªè·å–åˆ°æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®ï¼Œè¯·æ£€æŸ¥æŒ‡æ•°ä»£ç æˆ–APIæƒé™"
                    )
                    return 0

            # è·å–æœ€æ–°äº¤æ˜“æ—¥çš„æ•°æ®ï¼ˆé€šå¸¸æ˜¯å½“æœˆæœ€åä¸€ä¸ªäº¤æ˜“æ—¥ï¼‰
            latest_date = df["trade_date"].max()
            df = df[df["trade_date"] == latest_date]

            self.logger.info(
                f"æˆåŠŸè·å–{len(df)}åª{index_code}çš„æˆåˆ†è‚¡ï¼ˆ{latest_date}ï¼‰"
            )

            # é‡å‘½ååˆ—
            df = df.rename(columns={"con_code": "stock_code"})

            # æ·»åŠ æŒ‡æ•°ä»£ç 
            df["index_code"] = index_code
            df["is_current"] = True

            # ç¡®ä¿ weight åˆ—å­˜åœ¨
            if "weight" not in df.columns:
                df["weight"] = 0

            # æ·»åŠ æ•°æ®æºå’Œé‡‡é›†æ—¶é—´
            df["data_source"] = "tushare"
            df["collect_time"] = datetime.now()

            # åœ¨æ’å…¥æ•°æ®åº“å‰æ˜¾å¼é€‰æ‹©éœ€è¦çš„åˆ—
            needed_columns = [
                "index_code",
                "stock_code",
                "weight",
                "is_current",
                "data_source",
                "collect_time",
            ]
            df = df[needed_columns]

            # æ¸…ç†æ—§æ•°æ®
            self.delete_old_index_components(index_code)

            # æ’å…¥æ•°æ®åº“
            return self.insert_data(df, "IndexComponent")
        except Exception as e:
            self.logger.error(f"è·å–æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®å¤±è´¥: {e}")
            traceback.print_exc()  # æ‰“å°å®Œæ•´é”™è¯¯å †æ ˆï¼Œæ–¹ä¾¿è°ƒè¯•
            return 0

    def delete_old_index_components(self, index_code):
        """åˆ é™¤æ—§çš„æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®"""
        try:
            cursor = self.connection.cursor()
            query = "UPDATE IndexComponent SET is_current = FALSE WHERE index_code = %s"
            cursor.execute(query, (index_code,))
            self.connection.commit()
            self.logger.info(f"å·²å°†{index_code}çš„æ—§æˆåˆ†è‚¡æ ‡è®°ä¸ºéå½“å‰")
        except Exception as e:
            if self.connection:
                self.connection.rollback()
            self.logger.error(f"æ›´æ–°æ—§æŒ‡æ•°æˆåˆ†è‚¡çŠ¶æ€å¤±è´¥: {e}")
        finally:
            if cursor:
                cursor.close()

    def get_trading_calendar(
        self, exchange="SSE", start_date=None, end_date=None
    ) -> int:
        """è·å–äº¤æ˜“æ—¥å†"""
        if not start_date:
            # é»˜è®¤è·å–å½“å¹´å’Œä¸‹ä¸€å¹´çš„æ—¥å†
            today = datetime.now()
            start_date = f"{today.year}0101"

        if not end_date:
            today = datetime.now()
            end_date = f"{today.year + 1}1231"

        try:
            self.logger.info(f"è·å–{exchange}ä»{start_date}åˆ°{end_date}çš„äº¤æ˜“æ—¥å†...")

            df = self.pro.trade_cal(
                exchange=exchange, start_date=start_date, end_date=end_date
            )

            if df.empty:
                self.logger.warning(f"æœªè·å–åˆ°äº¤æ˜“æ—¥å†æ•°æ®")
                return 0

            # è½¬æ¢æ—¥æœŸæ ¼å¼
            df["cal_date"] = pd.to_datetime(df["cal_date"]).dt.date
            if "pretrade_date" in df.columns:
                df["pretrade_date"] = pd.to_datetime(df["pretrade_date"]).dt.date
            else:
                df["pretrade_date"] = None

            # æ·»åŠ æ•°æ®æºå’Œé‡‡é›†æ—¶é—´
            df["data_source"] = "tushare"
            df["collect_time"] = datetime.now()

            # æ’å…¥æ•°æ®åº“
            return self.insert_data(df, "TradingCalendar")
        except Exception as e:
            self.logger.error(f"è·å–äº¤æ˜“æ—¥å†å¤±è´¥: {e}")
            return 0

    def insert_data(
        self, df: pd.DataFrame, table_name: str, on_duplicate="update"
    ) -> int:
        """é€šç”¨æ•°æ®æ’å…¥æ–¹æ³•"""
        if df.empty:
            return 0

        try:
            # åœ¨æ’å…¥å‰å¤„ç† NaN å€¼ï¼Œå°†å…¶æ›¿æ¢ä¸º Noneï¼ˆå¯¹åº”MySQLä¸­çš„ NULLï¼‰
            df = df.replace({float("nan"): None, pd.NA: None})

            cursor = self.connection.cursor()

            # æ„å»ºSQL
            columns = df.columns.tolist()
            placeholders = ", ".join(["%s"] * len(columns))

            sql = f"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({placeholders})"

            if on_duplicate == "update":
                updates = [
                    f"{col}=VALUES({col})"
                    for col in columns
                    if col not in ["collect_time"]
                ]
                if updates:
                    sql += f" ON DUPLICATE KEY UPDATE {', '.join(updates)}"

            # æ‰§è¡Œæ‰¹é‡æ’å…¥
            data = [tuple(x) for x in df.values]
            cursor.executemany(sql, data)
            self.connection.commit()

            inserted = cursor.rowcount
            self.logger.info(f"æˆåŠŸæ’å…¥/æ›´æ–°{inserted}æ¡è®°å½•åˆ°{table_name}è¡¨")
            return inserted
        except Exception as e:
            if self.connection:
                self.connection.rollback()
            self.logger.error(f"æ’å…¥æ•°æ®åˆ°{table_name}è¡¨å¤±è´¥: {e}")
            traceback.print_exc()
            return 0
        finally:
            if cursor:
                cursor.close()

    def get_index_stocks(self, index_code="000300.SH") -> List[str]:
        """è·å–æŒ‡å®šæŒ‡æ•°çš„æˆåˆ†è‚¡åˆ—è¡¨"""
        try:
            cursor = self.connection.cursor()

            query = """
            SELECT stock_code FROM IndexComponent 
            WHERE index_code = %s AND is_current = TRUE
            """

            cursor.execute(query, (index_code,))
            result = cursor.fetchall()

            if not result:
                # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ï¼Œå°±ä»APIè·å–
                self.get_index_component(index_code)

                # å†æ¬¡æŸ¥è¯¢
                cursor.execute(query, (index_code,))
                result = cursor.fetchall()

            stock_list = [row[0] for row in result]
            self.logger.info(f"è·å–åˆ°{index_code}çš„{len(stock_list)}åªæˆåˆ†è‚¡")
            return stock_list
        except Exception as e:
            self.logger.error(f"è·å–æŒ‡æ•°æˆåˆ†è‚¡å¤±è´¥: {e}")
            return []
        finally:
            if cursor:
                cursor.close()


def load_config(config_file="config.json"):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        if not os.path.exists(config_file):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")

        with open(config_file, "r", encoding="utf-8") as f:
            config = json.load(f)

        # éªŒè¯å¿…éœ€å­—æ®µ
        if "db_password" not in config:
            raise ValueError("é…ç½®æ–‡ä»¶ç¼ºå°‘æ•°æ®åº“å¯†ç ")
        if "tushare_token" not in config:
            raise ValueError("é…ç½®æ–‡ä»¶ç¼ºå°‘Tushare token")

        return config
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        raise


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="é‡åŒ–äº¤æ˜“ç³»ç»Ÿå›æµ‹æ•°æ®å‡†å¤‡å·¥å…·")
    parser.add_argument(
        "--start", type=str, required=True, help="å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)"
    )
    parser.add_argument("--end", type=str, required=True, help="ç»“æŸæ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--stock", type=str, help="å‡†å¤‡å•åªè‚¡ç¥¨çš„å›æµ‹æ•°æ®")
    parser.add_argument(
        "--index", type=str, help="ä½¿ç”¨æŒ‡å®šæŒ‡æ•°çš„æˆåˆ†è‚¡ï¼Œä¾‹å¦‚ï¼š000300.SH (æ²ªæ·±300)"
    )
    parser.add_argument(
        "--config", type=str, default="config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )

    args = parser.parse_args()

    # éªŒè¯å¿…é¡»æä¾› --stock æˆ– --index å‚æ•°
    if not args.stock and not args.index:
        logger.error("å¿…é¡»æä¾› --stock æˆ– --index å‚æ•°")
        print("é”™è¯¯: å¿…é¡»æä¾› --stock æˆ– --index å‚æ•°")
        show_usage()
        return

    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config)

        # åˆ›å»ºæ•°æ®ç®¡ç†å™¨ï¼Œä¼ å…¥æ—¥æœŸèŒƒå›´
        stock_manager = StockDataManager(
            db_password=config["db_password"],
            tushare_token=config["tushare_token"],
            start_date=args.start,
            end_date=args.end,
        )

        # è¿æ¥æ•°æ®åº“
        stock_manager.connect_database()

        # å‡†å¤‡å›æµ‹æ•°æ®
        stock_codes = []

        if args.index:
            # ä½¿ç”¨æŒ‡å®šæŒ‡æ•°æˆåˆ†è‚¡
            logger.info(f"å‡†å¤‡æŒ‡æ•° {args.index} æˆåˆ†è‚¡çš„æ•°æ®...")
            stock_manager.get_index_component(args.index)  # ç¡®ä¿æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®æœ€æ–°
            stock_codes = stock_manager.get_index_stocks(args.index)

            if stock_codes:
                logger.info(
                    f"å°†ä¸ºæŒ‡æ•° {args.index} çš„ {len(stock_codes)} åªæˆåˆ†è‚¡å‡†å¤‡æ•°æ®"
                )
            else:
                logger.warning(
                    f"æœªæ‰¾åˆ°æŒ‡æ•° {args.index} çš„æˆåˆ†è‚¡ï¼Œè¯·æ£€æŸ¥æŒ‡æ•°ä»£ç æ˜¯å¦æ­£ç¡®"
                )
                return
        elif args.stock:
            # å•åªè‚¡ç¥¨æ¨¡å¼
            stock_codes = [args.stock]
            logger.info(f"å°†ä¸ºå•åªè‚¡ç¥¨ {args.stock} å‡†å¤‡æ•°æ®")

        # å‡†å¤‡æ‰€æœ‰å¿…è¦çš„æ•°æ®
        logger.info("å‡†å¤‡å›æµ‹æ‰€éœ€çš„å®Œæ•´æ•°æ®...")

        # 1. è·å–åŸºç¡€æ•°æ®è¡¨
        stock_manager.get_stock_basic()
        stock_manager.get_trading_calendar()

        # 2. è·å–å›æµ‹æœŸé—´çš„ä¼°å€¼æ•°æ®ï¼ˆä¸ºæ¯ä¸ªäº¤æ˜“æ—¥è·å–ï¼‰
        stock_manager.get_historical_stock_valuation()

        # 3. è·å–æœ€è¿‘å­£åº¦çš„è´¢åŠ¡æ•°æ®
        stock_manager.get_balance_sheet()
        stock_manager.get_income_statement()

        # 4. å‡†å¤‡è‚¡ç¥¨å¸‚åœºæ•°æ®
        results = stock_manager.prepare_backtest_data(stock_codes=stock_codes)

        logger.info(
            f"å›æµ‹æ•°æ®å‡†å¤‡å®Œæˆ - æˆåŠŸå¤„ç†{results['processed_stocks']}/{results['total_stocks']}åªè‚¡ç¥¨"
        )

        # æ ¹æ®æ¨¡å¼æ˜¾ç¤ºä¸åŒçš„å®Œæˆä¿¡æ¯
        if args.index:
            logger.info(f"æŒ‡æ•° {args.index} çš„æˆåˆ†è‚¡æ•°æ®å‡†å¤‡å®Œæˆï¼Œå¯ä»¥è¿›è¡Œå›æµ‹äº†")
        elif args.stock:
            logger.info(f"è‚¡ç¥¨ {args.stock} çš„æ•°æ®å‡†å¤‡å®Œæˆï¼Œå¯ä»¥è¿›è¡Œå›æµ‹äº†")

    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        logger.error(traceback.format_exc())
    finally:
        if "stock_manager" in locals():
            stock_manager.close_database()


def show_usage():
    """æ˜¾ç¤ºä½¿ç”¨è¯´æ˜"""
    print(
        """
ğŸ“– å›æµ‹æ•°æ®å‡†å¤‡å·¥å…·ä½¿ç”¨è¯´æ˜:

1ï¸âƒ£ å‡†å¤‡å•åªè‚¡ç¥¨æ•°æ®:
   python stock_data_fetcher.py --start 2024-01-01 --end 2024-08-31 --stock 600519.SH

2ï¸âƒ£ å‡†å¤‡æŒ‡æ•°æˆåˆ†è‚¡æ•°æ®:
   python stock_data_fetcher.py --start 2024-01-01 --end 2024-08-31 --index 000300.SH

ğŸ“‹ å‚æ•°è¯´æ˜:
   --start        : å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)ï¼Œå¿…é¡»æä¾›
   --end          : ç»“æŸæ—¥æœŸ (YYYY-MM-DD)ï¼Œå¿…é¡»æä¾›
   --stock        : å‡†å¤‡å•åªè‚¡ç¥¨çš„å›æµ‹æ•°æ®ï¼ˆå¿…é¡»æä¾›--stockæˆ–--indexï¼‰
   --index        : ä½¿ç”¨æŒ‡å®šæŒ‡æ•°çš„æˆåˆ†è‚¡ï¼Œä¾‹å¦‚ï¼š000300.SH (æ²ªæ·±300)
   --config       : é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºconfig.json
   --help         : æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

âš ï¸ æ³¨æ„äº‹é¡¹:
   - å¿…é¡»æä¾›å¼€å§‹æ—¥æœŸå’Œç»“æŸæ—¥æœŸ
   - å¿…é¡»æŒ‡å®šè‚¡ç¥¨ä»£ç (--stock)æˆ–æŒ‡æ•°ä»£ç (--index)
   - æ•°æ®å°†ä¿å­˜åˆ°æ•°æ®åº“ç›¸åº”çš„è¡¨ä¸­
   - ç¡®ä¿config.jsonä¸­åŒ…å«æ­£ç¡®çš„æ•°æ®åº“å¯†ç å’ŒTushareä»¤ç‰Œ
    """
    )


if __name__ == "__main__":
    main()
